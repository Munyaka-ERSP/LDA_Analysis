{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IpQ1nKQ1EEEL",
        "outputId": "cb8cc28b-75bd-4dca-d2c2-333021aceb67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (1.12.0)\n",
            "Collecting pandas>=2.0.0\n",
            "  Using cached pandas-2.2.0-cp311-cp311-win_amd64.whl (11.6 MB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (3.1.3)\n",
            "Requirement already satisfied: numexpr in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (2.9.0)\n",
            "Requirement already satisfied: funcy in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: gensim in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyLDAvis) (65.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\natas\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\~-ndas\\\\_libs\\\\algos.cp311-win_amd64.pyd'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "\n",
            "[notice] A new release of pip available: 22.3 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas==1.5.3\n",
            "  Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.0\n",
            "    Uninstalling pandas-2.2.0:\n",
            "      Successfully uninstalled pandas-2.2.0\n",
            "Successfully installed pandas-1.5.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
            "\n",
            "[notice] A new release of pip available: 22.3 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl (12.1 MB)\n",
            "     ---------------------------------------- 12.1/12.1 MB 8.7 MB/s eta 0:00:00\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
            "     -------------------------------------- 122.3/122.3 kB 7.0 MB/s eta 0:00:00\n",
            "Collecting thinc<8.3.0,>=8.1.8\n",
            "  Downloading thinc-8.2.3-cp311-cp311-win_amd64.whl (1.5 MB)\n",
            "     ---------------------------------------- 1.5/1.5 MB 10.4 MB/s eta 0:00:00\n",
            "Collecting wasabi<1.2.0,>=0.9.1\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
            "     ------------------------------------- 479.7/479.7 kB 15.1 MB/s eta 0:00:00\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "     ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
            "Collecting typer<0.10.0,>=0.3.0\n",
            "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "     ---------------------------------------- 45.9/45.9 kB 2.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (4.66.2)\n",
            "Collecting requests<3.0.0,>=2.13.0\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "     ---------------------------------------- 62.6/62.6 kB ? eta 0:00:00\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
            "  Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
            "     -------------------------------------- 394.8/394.8 kB 6.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: jinja2 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "     ------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.26.4)\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.16.2\n",
            "  Downloading pydantic_core-2.16.2-cp311-none-win_amd64.whl (1.9 MB)\n",
            "     ---------------------------------------- 1.9/1.9 MB 8.1 MB/s eta 0:00:00\n",
            "Collecting typing-extensions>=4.6.1\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
            "     ---------------------------------------- 99.9/99.9 kB ? eta 0:00:00\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "     ---------------------------------------- 61.6/61.6 kB ? eta 0:00:00\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
            "     -------------------------------------- 120.9/120.9 kB 6.9 MB/s eta 0:00:00\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "     ------------------------------------- 163.8/163.8 kB 10.2 MB/s eta 0:00:00\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
            "     ---------------------------------------- 6.6/6.6 MB 9.8 MB/s eta 0:00:00\n",
            "Collecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\natas\\appdata\\roaming\\python\\python311\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "     ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\natas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
            "Installing collected packages: cymem, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, langcodes, idna, cloudpathlib, charset-normalizer, certifi, catalogue, blis, annotated-types, typer, srsly, requests, pydantic-core, preshed, pydantic, confection, weasel, thinc, spacy\n",
            "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 certifi-2024.2.2 charset-normalizer-3.3.2 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 idna-3.6 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.1 pydantic-core-2.16.2 requests-2.31.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 typing-extensions-4.9.0 urllib3-2.2.0 wasabi-1.1.2 weasel-0.3.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\natas\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script weasel.exe is installed in 'c:\\Users\\natas\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script spacy.exe is installed in 'c:\\Users\\natas\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip available: 22.3 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction\n",
        "# TO USE:\n",
        "# Uncomment three !pip install lines\n",
        "# Run this cell ONLY\n",
        "# Runtime -> Restart session\n",
        "# Comment three !pip install lines\n",
        "# Run all\n",
        "\n",
        "# Andy can push\n",
        "\n",
        "%pip install pyLDAvis\n",
        "%pip install --user -U nltk\n",
        "%pip install \"pandas==1.5.3\"\n",
        "%pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu1xrf252p8h",
        "outputId": "615c4d49-eea2-4920-f2e7-add1a9d49369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.5.3\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "import nltk\n",
        "import pandas as pd #to work with csv files\n",
        "\n",
        "# pd.__version__ = 2.0.0\n",
        "print(pd.__version__)\n",
        "\n",
        "#Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "#spacy\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#vis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUzbqYoDkUPL",
        "outputId": "a05c8d73-157c-4264-ac71-576be5d0d3b4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mcheck_and_reconnect_drive\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Check if Google Drive is still connected\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# If not, reconnect it\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Then call this function every so often in your main script\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mcheck_and_reconnect_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mcheck_and_reconnect_drive\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# If not, reconnect it\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m     13\u001b[0m     drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "#The function that loops through the data to check for optimal parameters to input into the LDA model would take over 16 hours to run,\n",
        "#after a while the runtime disconnects, which makes the progress start from 0.\n",
        "#so the purpose of this function is to make sure the runtime doesnt disconnect, while training for optimal params to input to lda model.\n",
        "\n",
        "def check_and_reconnect_drive():\n",
        "    try:\n",
        "        # Check if Google Drive is still connected\n",
        "        os.listdir('/content/drive')\n",
        "    except:\n",
        "        # If not, reconnect it\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Then call this function every so often in your main script\n",
        "check_and_reconnect_drive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uFmthrM5V6o",
        "outputId": "dff3d57f-ac31-4157-8dc0-1bd790c4d641"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = stopwords.words(\"english\") #get stopwords from nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWl2ONo787sa"
      },
      "outputs": [],
      "source": [
        "#stopwords.extend(['hi', 'anymore', 'years', 'ago', 'almost', 'somehow', 'remember', 'wants', 'either', 'enough', 'much', 'making', 'simple', 'things', 'add', 'actual', 'say', 'etc', 'anything', 'days', 'long', 'another', 'ever', 'yes', 'th', 'ice', 'mightymouse', 'should', 'obviously', 'even', 'lol', 'stuff', 'never', 'old', 'yr', 'bit', 'like', 'aka', 'nonsense', 'ok', 'absolutely', 'op', 'weird', 'ass', 'fully', 'get', 'would', 'definitely', 'especially', 'still', 'else', 'something', 'often', 'kind', 'let', 'lot', 'able', 'non', 'pg', 'nice', 'trad', 'keep', 'massive', 'quickly', 'be', 'common', 'tv', 'fuckin', 'im', 'could', 'also', 'silly', 'ie', 'seem', 'seems', 'fucked'])#Add custom stop words\n",
        "stopwords.extend(['r/parenting', 'r/roblox', 'get'])#Add reddit paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRpoK3iEB6DX"
      },
      "outputs": [],
      "source": [
        "def wordcounts(text1):\n",
        "  fdist1 = FreqDist(text1)\n",
        "  print(fdist1)\n",
        "  fdist1.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXp-FdFNt7gc"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwfESysf57H_"
      },
      "outputs": [],
      "source": [
        "def load_data(file):\n",
        "  data = pd.read_csv(file)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTSUpkwF6ah1"
      },
      "outputs": [],
      "source": [
        "data = load_data('cumulative5.csv')\n",
        "filtered_data = data.drop(columns=['subreddit','date','timestamp'], axis=1)\n",
        "data = filtered_data.replace('\\n', ' ', regex=True)\n",
        "# data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmdgEwV9YpLv"
      },
      "outputs": [],
      "source": [
        "# data[data.text.str.contains('I am a bot')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTvUnK1IzgNM",
        "outputId": "42035f6f-b286-4461-e56e-b73a956c56c0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "data = data.loc[data.text.str.contains('I am a bot') == False, :]\n",
        "\n",
        "# Remove punctuation\n",
        "# data['text'] = data['text'].map(lambda x: re.sub(\"[,\\.!?/@#$^&*-_=+()%]\", \"\", x))\n",
        "data['text'] = data['text'].replace(r'http\\S+', '', regex=True)\n",
        "\n",
        "#data['text'] = data['text'].replace(r'a'+ u'\\u0301'+ 'U+20AC'+ 'U+2122', \"'\" , regex=True)\n",
        "\n",
        "data['text'] = data['text'].map(lambda x: re.sub('[%$:,\\.!?/-]', ' ', x))\n",
        "data['text'] = data['text'].map(lambda x: re.sub('\\(', ' ', x))\n",
        "data['text'] = data['text'].map(lambda x: re.sub('\\)', ' ', x))\n",
        "data['text'] = data['text'].replace(r'&\\S+', '', regex=True)\n",
        "data['text'] = data['text'].replace('\\\"', '', regex=True)\n",
        "\n",
        "# # Convert the titles to lowercase\n",
        "data['text'] = data['text'].map(lambda x: x.lower())\n",
        "\n",
        "pattern = r'[0-9]'\n",
        "data['text'] = data['text'].map(lambda x: re.sub(pattern, ' ', x))\n",
        "data['text'] = data['text'].map(lambda x: re.sub(' +', ' ', x))\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8eePq0Gxmtf",
        "outputId": "e66b9fbc-d0cd-42c0-f9ae-0681d7d3ad96"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZIQMdaaulEna",
        "outputId": "3032f721-d695-4170-fcad-3a8e7d10ef04"
      },
      "outputs": [],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cpMN8FZ3W5No",
        "outputId": "f9c09c46-5607-48e1-d0fb-6908f32b39b2"
      },
      "outputs": [],
      "source": [
        "data[data.text.str.contains('I am a bot')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "csMd6uvm6ukl",
        "outputId": "e2d0ca85-c027-450c-c113-04b0a1b44fc4"
      },
      "outputs": [],
      "source": [
        "data['text'][51]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1IqZjv9_Pcb",
        "outputId": "04b53f83-8dbe-411f-cbe6-6ba9a9338f9f"
      },
      "outputs": [],
      "source": [
        "!pip install clean-text\n",
        "from cleantext import clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxRt_wEr_wkF"
      },
      "outputs": [],
      "source": [
        "data = [clean(d, no_emoji=True) for d in data['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqJTjOEHAnmz"
      },
      "outputs": [],
      "source": [
        "data = remove_stopwords(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYFKmOGIA4Qx"
      },
      "outputs": [],
      "source": [
        "resultx = []\n",
        "for d in data:\n",
        "  resultx.append(' '.join(d))\n",
        "\n",
        "#resultxtokens = resultx.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJNDFYA0Pw5Y",
        "outputId": "72aa20e0-ff43-4ba5-f9e6-75ecd468da0c"
      },
      "outputs": [],
      "source": [
        "resultx[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz8We9sCDpPV",
        "outputId": "e39bd401-2491-4c63-ecfd-a48f106286a2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "words = [word_tokenize(i) for i in resultx]\n",
        "words2= [num for elem in words for num in elem]\n",
        "#words = nltk.tokenize.word_tokenize(resultx)\n",
        "\n",
        "#wordcounts(words2)\n",
        "fdist1 = FreqDist(words2)\n",
        "#print(fdist1)\n",
        "fdist1.most_common(50)\n",
        "\n",
        "\n",
        "#>>> # flatten a list using a listcomp with two 'for'\n",
        "#>>> vec = [[1,2,3], [4,5,6], [7,8,9]]\n",
        "#>>> [num for elem in vec for num in elem]\n",
        "#[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xisn0ly6HiOK"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "for d in data:\n",
        "  result.append(' '.join(d))\n",
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFGsHzqL1WeB"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "# data = filtered_data.review_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(result))\n",
        "\n",
        "# prints the first 30 words of the first tokenized review from the data_words list.\n",
        "# print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah4kOFw09mdw"
      },
      "outputs": [],
      "source": [
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwxlchWD81Tg"
      },
      "outputs": [],
      "source": [
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo7MOKefn6kt"
      },
      "outputs": [],
      "source": [
        "# # NO BIGRAMS\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# import spacy\n",
        "\n",
        "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# # Do lemmatization keeping only noun, adj, vb, adv\n",
        "# data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGmZgbPx9I2A",
        "outputId": "ca064a5a-1cd6-465b-c6ea-3b7f3d5e99b8"
      },
      "outputs": [],
      "source": [
        "# BIGRAMS DATA\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words)\n",
        "\n",
        "# Form Trigrams\n",
        "data_words_trigrams = make_trigrams(data_words)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svyyw3T_TWDY",
        "outputId": "062ce0a3-a02a-4bc5-c926-14f53bb314cd"
      },
      "outputs": [],
      "source": [
        "print(type(data_lemmatized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52HMZJNfK-6g"
      },
      "source": [
        "Remove Duplicate Words (TO REMOVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TsA85qZqam1"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "# def unique(d):\n",
        "#   UniqW = Counter(d)\n",
        "#   result = [*UniqW]\n",
        "#   # corpus = ' '.join(UniqW.keys())\n",
        "#   return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UK5UWvMEmhe"
      },
      "outputs": [],
      "source": [
        "# single_data = []\n",
        "# for d in data_lemmatized:\n",
        "#   for word in d:\n",
        "#     single_data.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V_DwEN_A0gk"
      },
      "outputs": [],
      "source": [
        "# unique_data = [unique(d) for d in data_lemmatized]\n",
        "# # unique_data = unique(single_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSsieaNDG4TQ",
        "outputId": "90222135-3d0e-41fb-826e-84d8c8159d08"
      },
      "outputs": [],
      "source": [
        "# print(type(unique_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBk1WzFhCMCE"
      },
      "outputs": [],
      "source": [
        "# unique_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYowXc6I5gYo",
        "outputId": "90609a11-3f4c-4c6a-b9aa-fd1db3697258"
      },
      "outputs": [],
      "source": [
        "from prompt_toolkit.completion import word_completer\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "# id2word = corpora.Dictionary(bag_of_words)\n",
        "\n",
        "corpus = []\n",
        "for text in data_lemmatized:\n",
        "  new = id2word.doc2bow(text)\n",
        "  corpus.append(new)\n",
        "\n",
        "print(corpus[0])\n",
        "\n",
        "print(data_words[:1][0][:30])\n",
        "# print(word_completer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKHoJVMrICnA"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=5,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True,\n",
        "                                       alpha=0.01,\n",
        "                                       eta=0.61)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "lwJxooAf0IV2",
        "outputId": "4af43a9d-d36c-4696-b6b5-187171bed83f"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=30)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1l1MVCTFxYG",
        "outputId": "9bf6a9b8-e820-48c4-dcf5-6c9dedc7e3f7"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fctISv2EHVRZ",
        "outputId": "262b3b73-5fe7-4e09-8504-71eddabfbcce"
      },
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Coherence Score\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "# coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, corpus=corpus, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA3X0xe_un0-"
      },
      "outputs": [],
      "source": [
        "# Get the topic distribution for each document\n",
        "document_topics = lda_model.get_document_topics(corpus)\n",
        "\n",
        "# Filter reviews that have topic 1 as their main topic\n",
        "reviews_in_topic_1 = [idx for idx, topics in enumerate(document_topics) if max(topics, key=lambda x: x[1])[0] == 1]\n",
        "\n",
        "i = 0\n",
        "# Print the reviews\n",
        "for idx in reviews_in_topic_1:\n",
        "    i += 1\n",
        "    print(' '.join(data_words[idx]))\n",
        "\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGD7bxby_HDV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "    lda_model = gensim.models.LdaModel(corpus=corpus,\n",
        "                                       id2word=dictionary,\n",
        "                                       num_topics=k,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       alpha=a,\n",
        "                                       eta=b,\n",
        "                                       per_word_topics=True)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    return coherence_model_lda.get_coherence()\n",
        "\n",
        "#data_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m7_xdHC-hPo",
        "outputId": "6faa1142-eec2-45fc-f478-f89c991ef7e5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        " # Topics range\n",
        "min_topics = 2\n",
        "max_topics = 11\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "# Alpha parameter\n",
        "#A list of alpha values to be tested.\n",
        "#It includes a range of values from 0.01 to 1 with a step size of 0.3, as well as two special values: 'symmetric' and 'asymmetric'.\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        " #assumes that all documents are equally likely to be a mixture of topics, resulting in a uniform distribution of topics across documents. So it maps rreviews to each topic evenly\n",
        " #It makes the model assume that every document should have roughly the same proportion of topics.\n",
        "# alpha.append('symmetric')\n",
        "# #allows for more variability in document-topic proportions.\n",
        "# #It doesn't enforce a uniform distribution and allows documents to have varying degrees of focus on different topics.\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# # Beta parameter\n",
        "# #A list of beta values to be tested. It follows a similar pattern to alpha, including a range and 'symmetric'.\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# # Validation sets:\n",
        "# # We have the corpus sets, which contains 2 versions of the corpus, 15% and 20%\n",
        "num_of_docs = len(corpus)\n",
        "\n",
        "# #A list containing two versions of the 15% corpus, one with 20% of the documents and another with 15% of the documents.\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.20)),\n",
        "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.15))]\n",
        "\n",
        "corpus_title = ['20% Corpus', '15% Corpus']\n",
        "\n",
        "#Stores the results of the LDA model with different hyperparameters.\n",
        "model_results = {'Validation_Set': [],\n",
        "                  'Topics': [],\n",
        "                  'Alpha': [],\n",
        "                  'Beta': [],\n",
        "                  'Coherence': []\n",
        "                 }\n",
        "\n",
        " # Can take a long time to run\n",
        "if 1 == 1:\n",
        "   #progress bar\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "\n",
        "     # iterate through validation corpus sets\n",
        "    for i in range(len(corpus_sets)):\n",
        "         # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "             # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                 # iterare through beta values\n",
        "                for b in beta:\n",
        "                     #reconnect the runtime\n",
        "                    check_and_reconnect_drive()\n",
        "                     # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,\n",
        "                                                  k=k, a=a, b=b)\n",
        "\n",
        "                     # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('/content/lda_tuning_results.csv', index=False)\n",
        "     # pd.DataFrame(model_results).to_csv('/content/drive/My Drive/Classes/lda_tuning_results.csv', index=False)\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OlYh9gFgvgq"
      },
      "outputs": [],
      "source": [
        "# if 1==1:\n",
        "# # pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
        "#     pd.DataFrame(model_results).to_csv('/content/drive/My Drive/Classes/lda_tuning_results.csv', index=False)\n",
        "#     pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "4m8kMDVthlHA",
        "outputId": "cbd62656-9469-468e-ee1c-97142b34da6b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the model results from the CSV file\n",
        "model_results_df = pd.read_csv('/content/lda_tuning_results.csv')\n",
        "\n",
        "# Filter the results for the 75% Corpus (you can change this if needed)\n",
        "# filtered_results = model_results_df[model_results_df['Validation_Set'] == '20% Corpus']\n",
        "\n",
        "# Group the results by the number of topics (Topics) and calculate the mean coherence score\n",
        "coherence_by_topics = model_results_df.groupby('Topics')['Coherence'].mean()\n",
        "\n",
        "# Plot the coherence score against the number of topics\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(coherence_by_topics.index, coherence_by_topics.values, marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Topics')\n",
        "plt.ylabel('Coherence Score (C_v)')\n",
        "plt.title('Coherence Score vs. Number of Topics')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6JTcRaZhwfJ",
        "outputId": "a257d6b4-6f47-4c2d-95ad-1f00ebb34a82"
      },
      "outputs": [],
      "source": [
        "data_topic_3 = model_results_df[model_results_df['Topics'] == 2]\n",
        "max_coherence = np.array(data_topic_3['Coherence'])\n",
        "print(max_coherence[4])\n",
        "# print(np.argmax(max_coherence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKhsArw_Ik3B"
      },
      "source": [
        "##Visualizing the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "8e-qsTyyIf-V",
        "outputId": "61d19bab-0544-4637-f384-ba5954102864"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=30)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oHuguVRlrZ2"
      },
      "outputs": [],
      "source": [
        "lda_model_tuned = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=2,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True,\n",
        "                                       alpha=0.31,\n",
        "                                       eta='symmetric')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "PPYjc5x3rmNh",
        "outputId": "1cea7564-45ae-48e6-f8bf-17b28b1f8eaa"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model_tuned, corpus, id2word, mds='mmds', R=30)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpu6N5jXCn5E",
        "outputId": "c942f103-9f51-4d20-9ace-725a529b1a9f"
      },
      "outputs": [],
      "source": [
        "# Get the topic distribution for each document\n",
        "document_topics = lda_model.get_document_topics(corpus)\n",
        "print(document_topics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5N3GXaBxAA4",
        "outputId": "84e2dfb5-15b0-49f3-bfe5-e6648c0f33b8"
      },
      "outputs": [],
      "source": [
        "# Filter reviews that have topic 1 as their main topic\n",
        "reviews_in_topic_1 = [idx for idx, topics in enumerate(document_topics) if max(topics, key=lambda x: x[1])[0] == 1]\n",
        "\n",
        "# Get the number of reviews in topic 1\n",
        "num_reviews_in_topic_1 = len(reviews_in_topic_1)\n",
        "\n",
        "print(f'There are {num_reviews_in_topic_1} reviews in topic 1.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuEUe7FVyOHU"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "#reviews_in_topic_1 = [idx for idx, topics in enumerate(document_topics) if max(topics, key=lambda x: x[1])[0] == 1]\n",
        "\n",
        "#mixture = [dict(lda_model[x]) for x in corpus]\n",
        "pd.DataFrame(reviews_in_topic_1).to_csv(\"topic_mixture.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxHTjwof7aoX",
        "outputId": "bc423e1d-8296-4fd0-9666-40bfc76dae51"
      },
      "outputs": [],
      "source": [
        "print(max(document_topics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5bodOS7dlM",
        "outputId": "c5e80cb5-3152-4018-97b4-b3aef15a3cef"
      },
      "outputs": [],
      "source": [
        "# Filter reviews that have topic 5 as their main topic\n",
        "reviews_in_topic_5 = [idx for idx, topics in enumerate(document_topics) if max(topics, key=lambda x: x[1])[0] == 5]\n",
        "\n",
        "# Get the number of reviews in topic 5\n",
        "num_reviews_in_topic_5 = len(reviews_in_topic_5)\n",
        "\n",
        "print(f'There are {num_reviews_in_topic_5} reviews in topic 5.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpkIGfMHAZSr",
        "outputId": "376bd1fa-f554-4bd3-aa1a-56d287732465"
      },
      "outputs": [],
      "source": [
        "datax= load_data('cumulative5.csv')\n",
        "filtered_data = datax.drop(columns=['subreddit','date','timestamp'], axis=1)\n",
        "datax= filtered_data.replace('\\n', ' ', regex=True)\n",
        "import re\n",
        "\n",
        "datax= datax.loc[datax.text.str.contains('I am a bot') == False, :]\n",
        "\n",
        "# Remove punctuation\n",
        "# data['text'] = data['text'].map(lambda x: re.sub(\"[,\\.!?/@#$^&*-_=+()%]\", \"\", x))\n",
        "datax['text'] = datax['text'].replace(r'http\\S+', '', regex=True)\n",
        "\n",
        "#data['text'] = data['text'].replace(r'a'+ u'\\u0301'+ 'U+20AC'+ 'U+2122', \"'\" , regex=True)\n",
        "\n",
        "datax['text'] = datax['text'].map(lambda x: re.sub('[%$:,\\.!?/-]', ' ', x))\n",
        "datax['text'] = datax['text'].map(lambda x: re.sub('\\(', ' ', x))\n",
        "datax['text'] = datax['text'].map(lambda x: re.sub('\\)', ' ', x))\n",
        "datax['text'] = datax['text'].replace(r'&\\S+', '', regex=True)\n",
        "datax['text'] = datax['text'].replace('\\\"', '', regex=True)\n",
        "\n",
        "# # Convert the titles to lowercase\n",
        "datax['text'] = datax['text'].map(lambda x: x.lower())\n",
        "\n",
        "pattern = r'[0-9]'\n",
        "datax['text'] = datax['text'].map(lambda x: re.sub(pattern, ' ', x))\n",
        "datax['text'] = datax['text'].map(lambda x: re.sub(' +', ' ', x))\n",
        "\n",
        "datax.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RG-HinbLDp9k",
        "outputId": "8a1e6d70-a768-44e4-d329-1b34df526340"
      },
      "outputs": [],
      "source": [
        "datax.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYyTmM6VDpnN"
      },
      "outputs": [],
      "source": [
        "datax['text'][3]\n",
        "type(datax['text'][3])\n",
        "\n",
        "resultx2 = []\n",
        "for d in datax['text']:\n",
        "  resultx2.append(d)\n",
        "\n",
        "#resultx2.head[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx-KjA4642T6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utX_jISo46a4"
      },
      "outputs": [],
      "source": [
        "# Get the comments without weird spaces\n",
        "\n",
        "\n",
        "i = 0\n",
        "# Print the reviews\n",
        "#filex=[\"\"]\n",
        "id=[\" \"]\n",
        "textx=[\" \"]\n",
        "comment=\"\"\n",
        "for i in datax['text']:\n",
        "    #i += 1\n",
        "    #print(idx)\n",
        "    #print(' '.join(data_words[ix]))\n",
        "    id.append(i)\n",
        "    textx.append(i)\n",
        "    # create a dictionary with the three lists\n",
        "dict = {'ID': id,'Text':textx}\n",
        "\n",
        "# create a Pandas DataFrame from the dictionary\n",
        "df = pd.DataFrame(dict)\n",
        "\n",
        "# write the DataFrame to a CSV file\n",
        "df.to_csv('Data.csv')\n",
        "\n",
        "#pd.DataFrame(filex).to_csv(\"topic_mixture1.csv\")\n",
        "#print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB-ZLPV77gwA"
      },
      "outputs": [],
      "source": [
        "# Get the topic distribution for each document\n",
        "document_topics = lda_model.get_document_topics(corpus)\n",
        "\n",
        "# Filter reviews that have topic 1 as their main topic\n",
        "reviews_in_topic_1_tuned = [idx for idx, topics in enumerate(document_topics) if max(topics, key=lambda x: x[1])[0] == 1]\n",
        "\n",
        "i = 0\n",
        "# Print the reviews\n",
        "filex=[\"\"]\n",
        "id=[\" \"]\n",
        "textx=[\" \"]\n",
        "comment=\"\"\n",
        "for idx in reviews_in_topic_1:\n",
        "    i += 1\n",
        "    #print(idx)\n",
        "    #print(' '.join(data_words[idx]))\n",
        "    filex.append(data_words[idx])\n",
        "    id.append(idx)\n",
        "    textx.append(resultx2[idx])\n",
        "    # create a dictionary with the three lists\n",
        "dict = {'top words': filex, 'ID': id,'Text':textx}\n",
        "\n",
        "# create a Pandas DataFrame from the dictionary\n",
        "df = pd.DataFrame(dict)\n",
        "\n",
        "# write the DataFrame to a CSV file\n",
        "df.to_csv('EmployeeData.csv')\n",
        "\n",
        "#pd.DataFrame(filex).to_csv(\"topic_mixture1.csv\")\n",
        "#print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk8dxshCz8eX"
      },
      "outputs": [],
      "source": [
        "# Get the topic distribution for each document\n",
        "document_topics_tuned = lda_model_tuned.get_document_topics(corpus)\n",
        "\n",
        "# Filter reviews that have topic 1 as their main topic\n",
        "reviews_in_topic_1_tuned = [idx for idx, topics in enumerate(document_topics) if max(topics, key=lambda x: x[1])[0] == 1]\n",
        "\n",
        "i = 0\n",
        "# Print the reviews\n",
        "for idx in reviews_in_topic_1_tuned:\n",
        "    i += 1\n",
        "    print(' '.join(data_words[idx]))\n",
        "\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZjX2yPE2Xzd"
      },
      "outputs": [],
      "source": [
        "# Filter reviews that have topic 1 as their main topic\n",
        "reviews_in_topic_2_tuned = [idx for idx, topics in enumerate(document_topics) if max(topics, key=lambda x: x[1])[0] == 2]\n",
        "\n",
        "i = 0\n",
        "# Print the reviews\n",
        "filex=[\"\"]\n",
        "id=[\" \"]\n",
        "textx=[\" \"]\n",
        "comment=\"\"\n",
        "for idx in reviews_in_topic_2_tuned:\n",
        "    i += 1\n",
        "    #print(idx)\n",
        "    #print(' '.join(data_words[idx]))\n",
        "    filex.append(data_words[idx])\n",
        "    id.append(idx)\n",
        "    textx.append(resultx2[idx])\n",
        "    # create a dictionary with the three lists\n",
        "dict = {'top words': filex, 'ID': id,'Text':textx}\n",
        "\n",
        "# create a Pandas DataFrame from the dictionary\n",
        "df = pd.DataFrame(dict)\n",
        "\n",
        "# write the DataFrame to a CSV file\n",
        "df.to_csv('EmployeeData2.csv')\n",
        "\n",
        "#pd.DataFrame(filex).to_csv(\"topic_mixture1.csv\")\n",
        "#print(i)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
